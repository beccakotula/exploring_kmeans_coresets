---
title: "Exploring the Power of Coresets for $k$-Means Clustering"
subtitle: |
    | Johns Hopkins University  
    | EN.625.740 Data Mining
author: "Rebecca Kotula"
format: 
    revealjs:
        incremental: true
editor: visual
---

```{r setup_imports, include=FALSE}
library(ggplot2)
library(GGally)
library(ggpubr)
library(Rtsne)
library(kableExtra)
library(MASS) 
library(tidyverse)
library(Rfast)

library(reticulate)
use_virtualenv("/Users/rebeccakotula/Documents/jhu/datamining/dm")
np <- import("numpy")
coresets <- import("coresets")

seed <- 1234

data(iris)

### K-means helper functions 
predict.kmeans <- function(km.object, newdata){
    centers <- km.object$centers
    n.centers <- nrow(centers)
    dist.mat <- as.matrix(dist(rbind(centers, newdata)))
    dist.mat <- dist.mat[-seq(n.centers), seq(n.centers)]
    max.col(-dist.mat)
}

euclidean.dist <- function(a, b){
    sqrt(sum((a - b)^2))
}

kmeans.tot.withinss <- function(km.object, data){
    centers <- km.object$centers
    dist.mat <- data.frame(matrix(NA, nrow = nrow(data), ncol = nrow(centers)))
    for (i in 1:nrow(centers)){
        dist.mat[,i] <- apply(data, 1, function(x) euclidean.dist(x,centers[i,])) 
    }
    dists <- apply(dist.mat, 1, function(x) min(x))
    sum(dists^2)
}

relabel.clusters <- function(km.object, data.x, data.y, preds){
    centers <- km.object$centers
    n.centers <- nrow(centers)
    dist.mat <- as.matrix(dist(rbind(centers, data.x)))
    dist.mat <- dist.mat[-seq(n.centers), seq(n.centers)]
    min.idxs <- as.integer(colMins(dist.mat)) #rownames(dist.mat)[colMins(dist.mat)]
    clusts <- seq(1, n.centers, by=1)
    labels <- data.y[min.idxs]
    
    preds <- as.data.frame(preds)
    for (i in 1:n.centers){
        preds <- data.frame(preds)
        preds[preds$preds==i,] <- paste0(labels[i])
    }
    return(preds)
}
```

## Agenda

-   Introduction

-   Project Overview

-   Background

-   Datasets

-   Coresets

-   Results

## Coresets???

-   **Coreset**: a weighted subset of a dataset which yields provably comparable accuracy to the original dataset for a given objective

-   Coresets are problem-specific; requires a cost function to be optimized

-   Benefits: reduce computational cost, applicable to streaming data

## Project Overview

1.  Demonstrate effectiveness of constructing coresets for $k$-means clustering

2.  Compare results on a small vs. large dataset

3.  Compare results from different sized coresets

# Background

## k-Means Clustering

Given a dataset $X$, compute a set of $k$ centers.

Call this set of centers $Q$, with $|Q|=k$.

Centers are chosen to optimize $$cost(X, Q) =\sum_{x\in X} \min_{q\in Q} \lVert x-q\rVert_2^2$$

Intuitively: we want to minimize the sum of squared distances from each point $x$ to the nearest cluster center in $Q$ (the set of centers).

## k-Means Clustering

1.  Initialize cluster centers

2.  Assign points to closest cluster center

3.  Compute new centers (mean of each cluster)

4.  Repeat until convergence

## k-Means Coresets

Our cost function is:

$$cost(X, Q) =\sum_{x\in X} \min_{q\in Q} \lVert x-q\rVert_2^2$$

$C$ is an $\epsilon-coreset$ of $X$ if for all $Q\in \mathbf{Q}$:$$\lvert cost(X, Q) - cost(C, Q)\rvert \leq \epsilon * cost(X, Q)$$

## Coreset Construction

There are many approaches to coreset construction.

-   Naive approach: Uniform sampling

    -   Requires still relatively larger-sized coresets to have "provably comparable accuracy"

-   More informed: Bounded point movement

    -   Points are moved (within boundaries), merged with nearby points, and weighted accordingly

-   Used in this project: Importance & sensitivity-based sampling

## Coreset Construction

-   Importance & sensitivity-based sampling

    -   Points are sampled in a way that is proportional to their impact on the cost of any given candidate solution

    -   Sensitivity defined as:

        $$ sens(x) = sup_Q \frac{\min_{q\in Q} \lVert x-q\rVert_2^2}{cost_{X}(Q)}$$

    -   Intuitively: sensitivity of a point is the maximum share of a point in the cost function

# Datasets

## Dataset 1: Iris Dataset

*We all know it... we all love it...*

`r nrow(iris)` observations of 4 features: `Sepal.Length, Sepal.Width, Petal.Length, Petal.Width`.

Three class labels: `Setosa`, `Versicolor`, and `Virginica`.

```{r iris-class-dist, echo=FALSE}
table(iris$Species) %>% 
    kbl() %>%
    kable_paper(full_width = F)%>%
  kable_styling(latex_options="HOLD_position")
```

## Dataset 1: Iris Dataset

```{r, message=FALSE,echo=FALSE}
ggpairs(iris,title = "Iris Dataset Features",  columns=1:4, ggplot2::aes(colour = Species))+ theme_minimal()
```

## Dataset 1: Iris Dataset

```{r, include=FALSE, message=FALSE}
train.size <- floor(0.75 * nrow(iris))
set.seed(seed)
train.ind <- sample(seq_len(nrow(iris)), size = train.size)

train.iris <- iris[train.ind, ]
test.iris <- iris[-train.ind, ]
```

```{r, echo=FALSE}
g <- ggplot(train.iris, aes(x=Petal.Length, y = Petal.Width, col=Species)) + 
    geom_point() + 
    theme_minimal() +
    ggtitle("Iris Training Dataset with True Class Labels")
g
```

## Dataset 2: Simulated 5-D Gaussians

```{r, include=FALSE}
# set seed and create data vectors 
set.seed(seed) 
sample.size <- 4000  
```

```{r simulated-data}
class1.mu <- c(10, 5, 7, 9, 20)   
class2.mu <- c(9, 3, 4, 5, 15)   
class3.mu <- c(3, 2, 1, 4, 9)   
class4.mu <- c(5, 5, 5, 5, 5)   

n <- 5 
A1 <- matrix(runif(n^2)*2-1, ncol=n) 
sigma1 <- t(A1) %*% A1
A2 <- matrix(runif(n^2)*4-2, ncol=n) 
sigma2 <- t(A2) %*% A2
A3 <- matrix(runif(n^2)*3-2, ncol=n) 
sigma3 <- t(A3) %*% A3
A4 <- matrix(runif(n^2)*2-2, ncol=n) 
sigma4 <- t(A4) %*% A4
```

``` r
class1.mu <- c(10, 5, 7, 9, 20)   
class2.mu <- c(9, 3, 4, 5, 15)   
class3.mu <- c(3, 2, 1, 4, 9)   
class4.mu <- c(5, 5, 5, 5, 5)   

n <- 5 
A1 <- matrix(runif(n^2)*2-1, ncol=n) 
sigma1 <- t(A1) %*% A1
A2 <- matrix(runif(n^2)*4-2, ncol=n) 
sigma2 <- t(A2) %*% A2
A3 <- matrix(runif(n^2)*3-2, ncol=n) 
sigma3 <- t(A3) %*% A3
A4 <- matrix(runif(n^2)*2-2, ncol=n) 
sigma4 <- t(A4) %*% A4

class1.dat <- mvrnorm(n = sample.size, mu = class1.mu, Sigma = sigma1)
class2.dat <- mvrnorm(n = sample.size, mu = class2.mu, Sigma = sigma2)
class3.dat <- mvrnorm(n = sample.size, mu = class3.mu, Sigma = sigma3)
class4.dat <- mvrnorm(n = sample.size, mu = class4.mu, Sigma = sigma4)
```

```{r gen-sim-data, include=FALSE}
# create multivariate normal distribution 
class1.dat <- data.frame(mvrnorm(n = sample.size, 
                               mu = class1.mu,  
                               Sigma = sigma1))
class2.dat <- data.frame(mvrnorm(n = sample.size, 
                               mu = class2.mu,  
                               Sigma = sigma2)) 
class3.dat <- data.frame(mvrnorm(n = sample.size, 
                               mu = class3.mu,  
                               Sigma = sigma3))
class4.dat <- data.frame(mvrnorm(n = sample.size, 
                               mu = class4.mu,  
                               Sigma = sigma4))

class1.dat$class <- "a"
class2.dat$class <- "b"
class3.dat$class <- "c"
class4.dat$class <- "d"

sim.data <- rbind(class1.dat,rbind(class2.dat,rbind(class3.dat, class4.dat)))
sim.mat <- as.matrix(sim.data[,-6]) 

# Add Gaussian Noise 
sim.sd <- apply(sim.mat, 1, sd)
sim.dat <- sim.mat + rnorm(length(sim.mat), mean = 0, sd = sim.sd * 0.05)
sim.data[,1:5] <- sim.dat
```

(plus some additional Gaussian noise)

## Dataset 2: Simulated 5-D Gaussians

There are `r nrow(sim.data)` observations in this dataset and the class distribution is:

```{r dist-sim-data, echo=FALSE}
table(sim.data$c) %>%     
    kbl() %>%    
    kable_paper(full_width = F)%>%  kable_styling(latex_options="HOLD_position")
```

## Dataset 2: Simulated 5-D Gaussians

```{r, include=FALSE, message=FALSE}
train.size.sim <- floor(0.75 * nrow(sim.data))
set.seed(seed)
train.ind.sim <- sample(seq_len(nrow(sim.data)), size = train.size.sim)

train.sim <- sim.data[train.ind.sim, ]
test.sim <- sim.data[-train.ind.sim, ]

set.seed(seed)
train.sim.mat <- as.matrix(train.sim[,-6])
tsne.sim.out <- Rtsne(train.sim.mat, check_duplicates = FALSE)
```

```{r, echo=FALSE}
tsne.sim.plot <- data.frame(x = tsne.sim.out$Y[,1],
                        y = tsne.sim.out$Y[,2])
g.sim.tsne <- ggplot(tsne.sim.plot) +
    geom_point(aes(x=x, y=y, col = as.factor(train.sim[,6]))) +
    theme_minimal() +
    labs(color='class') +
    ggtitle("t-SNE Representation of Simulated Training Dataset")
g.sim.tsne
```

# Coresets

Using `coresets` package from PyPi, which implements importance sampling

## Coreset: Iris

```{r, include=FALSE, message=FALSE}
py_set_seed(seed, disable_hash_randomization = TRUE)
X <- as.matrix(train.iris[,c("Petal.Length", "Petal.Width")])
cs <- coresets$KMeansCoreset(X)
coreset_gen <- cs$generate_coreset(as.integer(nrow(X)*0.1))

train.iris.coreset <- data.frame(coreset_gen[[1]])
train.iris.coreset.weights <- coreset_gen[[2]]
colnames(train.iris.coreset) <- c("Petal.Length", "Petal.Width")
```

10% coreset- `r nrow(train.iris.coreset)` observations

```{r iris-coreset-plot, echo=FALSE}
c <- ggplot() + geom_point(data=train.iris, aes(x=Petal.Length, y = Petal.Width)) + 
    geom_point(data=cbind(train.iris.coreset,train.iris.coreset.weights), aes(x=Petal.Length,y=Petal.Width, size=train.iris.coreset.weights), alpha=0.6,col='red')+
    theme_minimal() + 
    ggtitle("Iris Training Dataset with Coreset in Red")
c
```

## Coreset: Simulated

```{r sim-coreset, include = FALSE}
# Generate Coreset
py_set_seed(seed, disable_hash_randomization = TRUE)
X.sim <- as.matrix(train.sim[,-6])
cs.sim <- coresets$KMeansCoreset(X.sim, n_clusters = as.integer(4), random_state = as.integer(1234))
coreset_gen.sim <- cs.sim$generate_coreset(as.integer(nrow(X.sim)*0.01))

train.sim.coreset <- data.frame(coreset_gen.sim[[1]])
train.sim.coreset.weights <- coreset_gen.sim[[2]]

# Which indices from original dataset are in the coreset: 
train.sim.list <- paste(train.sim[,1], train.sim[,2], train.sim[,3], train.sim[,4], train.sim[,5])
train.sim.coreset.list <- paste(train.sim.coreset[,1], train.sim.coreset[,2], train.sim.coreset[,3], train.sim.coreset[,4], train.sim.coreset[,5])
coreset.ind <- which(train.sim.list %in% train.sim.coreset.list)
```

1% coreset- `r nrow(train.sim.coreset)` observations

```{r, echo=FALSE, fig.width=8,fig.height=5}
c.sim <- ggplot() + 
    geom_point(data=tsne.sim.plot, aes(x=x, y=y), alpha=0.6) + 
    geom_point(data=tsne.sim.plot[coreset.ind,], aes(x=x,y=y), col='red')+ 
    theme_minimal() + 
    ggtitle("Simulated Training Dataset with Coreset in Red")
c.sim
```

# Results

## Clustering Results: Iris

```{r, include=FALSE}
set.seed(seed)
km <- kmeans(train.iris[,c("Petal.Length", "Petal.Width")], 3, nstart = 25)

preds <- km$cluster
preds <- relabel.clusters(km, 
                          iris[,c("Petal.Length", "Petal.Width")], 
                          iris[,"Species"], 
                          preds)

k <- ggplot(cbind(train.iris, preds), aes(x=Petal.Length, y = Petal.Width, col=preds)) + 
    geom_point()+ 
    theme_minimal() + 
    ggtitle("K-Means: Iris Training Set") +
    labs(color='Predicted Species') + 
    theme(legend.position = "none")

set.seed(seed)
km.c <- kmeans(train.iris.coreset[,1:2], 3, nstart = 25)
preds.c <- km.c$cluster
preds.c <- relabel.clusters(km.c, 
                          iris[,c("Petal.Length", "Petal.Width")], 
                          iris[,"Species"], 
                          preds.c)

train.iris.coreset["Species.Pred"] <- preds.c

k.c <- ggplot(train.iris.coreset, aes(col=Species.Pred)) +
    geom_point(aes(x=Petal.Length,y=Petal.Width))+ 
    theme_minimal() + 
    labs(color='Predicted Species') +
    ggtitle("K-Means: Iris Coreset")

c <- ggplot() + geom_point(data=iris, aes(x=Petal.Length, y = Petal.Width, col=Species)) + 
    geom_point(aes(x=km$centers[,1],y=km$centers[,2]), col='blue', shape=15, size=3)+
    geom_point(aes(x=km.c$centers[,1],y=km.c$centers[,2]), col='red', shape=15, size=3)+ 
    theme_minimal() +
    ggtitle("K-means Centers on Full Iris Dataset (True Labels)") 
```

```{r, echo=FALSE}
ggarrange(k, k.c, ncol=2, common.legend = TRUE, legend="bottom")
```

```{r iris-test-accuracy, include=FALSE}
set.seed(seed)
test.preds <- predict.kmeans(km, test.iris[,c("Petal.Length","Petal.Width")])
test.preds <- relabel.clusters(km, 
                          iris[,c("Petal.Length", "Petal.Width")], 
                          iris[,"Species"], 
                          test.preds)

acc.test <- sum(test.preds$preds==test.iris$Species)/nrow(test.iris)

set.seed(seed)
test.preds.c <- predict.kmeans(km.c, test.iris[,c("Petal.Length","Petal.Width")])
test.preds.c <- relabel.clusters(km.c, 
                          iris[,c("Petal.Length", "Petal.Width")], 
                          iris[,"Species"], 
                          test.preds.c)

acc.test.c <- sum(test.preds.c$preds==test.iris$Species)/nrow(test.iris)

acc <- c(acc.test, acc.test.c)
ss <- c(kmeans.tot.withinss(km,iris[,3:4]), kmeans.tot.withinss(km.c,iris[,3:4]))
train.set <- c("full Iris", "10% Iris coreset")
```

```{r, echo=FALSE}
data.frame(train.set =train.set, test.accuracy = acc, tot.within.ss = ss) %>% 
    kbl() %>%
    kable_paper(full_width = F)%>%
    kable_styling(latex_options="HOLD_position")
```

## Clustering Results: Iris

```{r, echo=FALSE}
c
```

## Clustering Results: Simulated Dataset

```{r, echo=FALSE}
set.seed(seed)
km.sim<- kmeans(train.sim[,-6], 4, nstart = 25)
preds.sim <- km.sim$cluster
preds.sim <- relabel.clusters(km.sim, train.sim[,-6], train.sim[,6], preds.sim)

set.seed(seed)
km.c.sim <- kmeans(train.sim.coreset, 4, nstart = 25)
preds.c.sim <- km.c.sim$cluster
preds.c.sim <- relabel.clusters(km.c.sim, 
                                train.sim[,-6], 
                                train.sim[,6], 
                                preds.c.sim)

set.seed(seed)
test.sim.preds <- predict.kmeans(km.sim, test.sim[,-6])
test.sim.preds <- relabel.clusters(km.sim, 
                                   train.sim[,-6], 
                                   train.sim[,6], 
                                   test.sim.preds)

acc.test.sim <- sum(test.sim.preds$preds==test.sim[,6])/nrow(test.sim)

set.seed(seed)
test.sim.c.preds <- predict.kmeans(km.c.sim, test.sim[,-6])
test.sim.c.preds <- relabel.clusters(km.c.sim, 
                                   train.sim[,-6], 
                                   train.sim[,6], 
                                   test.sim.c.preds)

acc.test.c.sim <- sum(test.sim.c.preds$preds==test.sim[,6])/nrow(test.sim)


acc.sim <- c(acc.test.sim, acc.test.c.sim)
ss.sim <- c(kmeans.tot.withinss(km.sim, sim.data[,-6]), kmeans.tot.withinss(km.c.sim, sim.data[,-6]))
train.set.sim <- c("full sim", "1% sim coreset")

data.frame(train.set =train.set.sim, test.accuracy = acc.sim, tot.within.ss = ss.sim) %>% 
    kbl() %>%
    kable_paper(full_width = F)%>%
    kable_styling(latex_options="HOLD_position")
```

## Coreset Size Experiments

```{r coreset-sizes, echo=FALSE}
sizes <- c(0.01, 0.04, 0.05, 0.1, 0.25)
iris.sizes <- as.integer(nrow(train.iris)*sizes)
sim.sizes <- as.integer(nrow(train.sim)*sizes)
size.table <- data.frame(size = sizes, 
                         train.iris = iris.sizes, 
                         train.sim = sim.sizes
                         )
size.table <- rbind(size.table, c("original", nrow(train.iris), nrow(train.sim)))
size.table %>% 
    kbl() %>%
    kable_paper(full_width = F)%>%
  kable_styling(latex_options="HOLD_position")
```

## Coreset Sizes: Iris

```{r, echo=FALSE}
sizes <- c(0.01,0.04, 0.05, 0.1, 0.25)
py_set_seed(seed, disable_hash_randomization = TRUE)
coreset.04 <- data.frame((cs$generate_coreset(as.integer(nrow(X)*0.04)))[[1]])
coreset.05 <- data.frame((cs$generate_coreset(as.integer(nrow(X)*0.05)))[[1]])
coreset.1 <- data.frame((cs$generate_coreset(as.integer(nrow(X)*0.1)))[[1]])
coreset.25 <- data.frame((cs$generate_coreset(as.integer(nrow(X)*0.25)))[[1]])

colnames(coreset.04) <- c("Petal.Length", "Petal.Width")
colnames(coreset.05) <- c("Petal.Length", "Petal.Width")
colnames(coreset.1) <- c("Petal.Length", "Petal.Width")
colnames(coreset.25) <- c("Petal.Length", "Petal.Width")

set.seed(seed)
km.c.04<- kmeans(coreset.04, 3, nstart = 25)
km.c.05<- kmeans(coreset.05, 3, nstart = 25)
km.c.1<- kmeans(coreset.1, 3, nstart = 25)
km.c.25<- kmeans(coreset.25, 3, nstart = 25)

test.preds.04 <- predict.kmeans(km.c.04, test.iris[,c("Petal.Length","Petal.Width")])
test.preds.05 <- predict.kmeans(km.c.05, test.iris[,c("Petal.Length","Petal.Width")])
test.preds.1 <- predict.kmeans(km.c.1, test.iris[,c("Petal.Length","Petal.Width")])
test.preds.25 <- predict.kmeans(km.c.25, test.iris[,c("Petal.Length","Petal.Width")])

test.preds.04 <- relabel.clusters(km.c.04,
                               iris[,c("Petal.Length", "Petal.Width")],
                               iris[,"Species"],
                               test.preds.04)
test.preds.05 <- relabel.clusters(km.c.05,
                               iris[,c("Petal.Length", "Petal.Width")],
                               iris[,"Species"],
                               test.preds.05)
test.preds.1 <- relabel.clusters(km.c.1,
                               iris[,c("Petal.Length", "Petal.Width")],
                               iris[,"Species"],
                               test.preds.1)
test.preds.25 <- relabel.clusters(km.c.25,
                               iris[,c("Petal.Length", "Petal.Width")],
                               iris[,"Species"],
                               test.preds.25)

test.acc.04<- sum(test.preds.04$preds==test.iris$Species)/nrow(test.iris)
test.acc.05<- sum(test.preds.05$preds==test.iris$Species)/nrow(test.iris)
test.acc.1<- sum(test.preds.1$preds==test.iris$Species)/nrow(test.iris)
test.acc.25<- sum(test.preds.25$preds==test.iris$Species)/nrow(test.iris)

test.acc <- c("--", test.acc.04, test.acc.05, test.acc.1, test.acc.25)

totwss.04 <- kmeans.tot.withinss(km.c.04,iris[,3:4])
totwss.05 <- kmeans.tot.withinss(km.c.05,iris[,3:4])
totwss.1 <- kmeans.tot.withinss(km.c.1,iris[,3:4])
totwss.25 <- kmeans.tot.withinss(km.c.25,iris[,3:4])

totwss <- c("--", totwss.04, totwss.05, totwss.1, totwss.25)

iris.sweep <- data.frame(coreset.size = sizes, test.accuracy = test.acc, tot.w.ss = totwss)
iris.sweep <- rbind(iris.sweep, c("full", acc.test, ss[1]))
iris.sweep %>% 
    kbl(caption="Iris Dataset Metrics") %>%
    kable_paper(full_width = F)%>%
    kable_styling(latex_options="HOLD_position")
```

## Coreset Sizes: Simulated Dataset

```{r simulated-coreset-size-sweep, echo=FALSE}
py_set_seed(seed, disable_hash_randomization = TRUE)
coreset.01.sim <- data.frame((cs.sim$generate_coreset(as.integer(nrow(X.sim)*0.01)))[[1]])
coreset.04.sim <- data.frame((cs.sim$generate_coreset(as.integer(nrow(X.sim)*0.04)))[[1]])
coreset.05.sim <- data.frame((cs.sim$generate_coreset(as.integer(nrow(X.sim)*0.05)))[[1]])
coreset.1.sim <- data.frame((cs.sim$generate_coreset(as.integer(nrow(X.sim)*0.1)))[[1]])
coreset.25.sim <- data.frame((cs.sim$generate_coreset(as.integer(nrow(X.sim)*0.25)))[[1]])

set.seed(seed)
km.c.01.sim <- kmeans(coreset.01.sim, 4, nstart = 25)
km.c.04.sim <- kmeans(coreset.04.sim, 4, nstart = 25)
km.c.05.sim <- kmeans(coreset.05.sim, 4, nstart = 25)
km.c.1.sim <- kmeans(coreset.1.sim, 4, nstart = 25)
km.c.25.sim <- kmeans(coreset.25.sim, 4, nstart = 25)

test.preds.01.sim <- predict.kmeans(km.c.01.sim, test.sim[,-6])
test.preds.04.sim <- predict.kmeans(km.c.04.sim, test.sim[,-6])
test.preds.05.sim <- predict.kmeans(km.c.05.sim, test.sim[,-6])
test.preds.1.sim <- predict.kmeans(km.c.1.sim, test.sim[,-6])
test.preds.25.sim <- predict.kmeans(km.c.25.sim, test.sim[,-6])

test.preds.01.sim <- relabel.clusters(km.c.01.sim,
                                test.sim[,-6],
                                test.sim[,6],
                               test.preds.01.sim)
test.preds.04.sim <- relabel.clusters(km.c.04.sim,
                                test.sim[,-6],
                                test.sim[,6],
                               test.preds.04.sim)
test.preds.05.sim <- relabel.clusters(km.c.05.sim,
                                test.sim[,-6],
                                test.sim[,6],
                               test.preds.05.sim)
test.preds.1.sim <- relabel.clusters(km.c.1.sim,
                                test.sim[,-6],
                                test.sim[,6],
                               test.preds.1.sim)
test.preds.25.sim <- relabel.clusters(km.c.25.sim,
                                test.sim[,-6],
                                test.sim[,6],
                               test.preds.25.sim)

test.acc.01.sim <- sum(test.preds.01.sim$preds==test.sim[,6])/nrow(test.sim)
test.acc.04.sim <- sum(test.preds.04.sim$preds==test.sim[,6])/nrow(test.sim)
test.acc.05.sim <- sum(test.preds.05.sim$preds==test.sim[,6])/nrow(test.sim)
test.acc.1.sim <- sum(test.preds.1.sim$preds==test.sim[,6])/nrow(test.sim)
test.acc.25.sim <- sum(test.preds.25.sim$preds==test.sim[,6])/nrow(test.sim)

test.acc <- c(test.acc.01.sim, 
              test.acc.04.sim, 
              test.acc.05.sim, 
              test.acc.1.sim, 
              test.acc.25.sim)

totwss.01.sim <- kmeans.tot.withinss(km.c.01.sim,sim.data[,-6])
totwss.04.sim <- kmeans.tot.withinss(km.c.04.sim,sim.data[,-6])
totwss.05.sim <- kmeans.tot.withinss(km.c.05.sim,sim.data[,-6])
totwss.1.sim <- kmeans.tot.withinss(km.c.1.sim,sim.data[,-6])
totwss.25.sim <- kmeans.tot.withinss(km.c.25.sim,sim.data[,-6])

totwss <- c(totwss.01.sim, totwss.04.sim, totwss.05.sim, totwss.1.sim, totwss.25.sim)

sim.sweep <- data.frame(coreset.size = sizes, test.accuracy = test.acc, tot.w.ss = totwss)
sim.sweep <- rbind(sim.sweep, c("full", acc.test.sim, ss.sim[1]))

sim.sweep %>% 
    kbl(caption="Simulated Dataset Metrics") %>%
    kable_paper(full_width = F)%>%
    kable_styling(latex_options="HOLD_position")
```

# Conclusion

Please see my paper and github repo for my references and code!

\
<https://github.com/beccakotula/exploring_kmeans_coresets>
